# Titanic â€” Machine Learning from Disaster

Baseline com Gradient Boosting (CV accuracy: 0.836).

## Como reproduzir
1. Python 3.10 + venv
2. `pip install -r requirements.txt`
3. Baixar `train.csv` e `test.csv` do Kaggle e colocar em `data/`
4. Rodar `notebooks/01_eda.ipynb`

## Estrutura
- `notebooks/01_eda.ipynb` â€” EDA + baseline
- `outputs/submission_baseline.csv` â€” primeira submissÃ£o
- `requirements.txt` â€” dependÃªncias

## ðŸ“Š Resumo Final â€“ CompetiÃ§Ã£o Titanic Kaggle

**Objetivo:** Melhorar o *recall* e o equilÃ­brio geral das mÃ©tricas, mantendo consistÃªncia no pipeline e explorando diferentes estratÃ©gias.

| # | EstratÃ©gia | MÃ©tricas CV (principais) | Threshold | PontuaÃ§Ã£o Kaggle |
|---|------------|--------------------------|-----------|------------------|
| 1 | Baseline (GradBoost melhor modelo inicial) | F1=0.77 aprox., Recall=0.79 | default | **0.75358** |
| 2 | OtimizaÃ§Ã£o de F2 (LogReg) | F2=0.803, Recall=0.836 | 0.397 | 0.72009 |
| 3 | Ajuste de threshold p/ Recallâ‰ˆ0.80 | F1â‰ˆ0.774, Recall=0.801 | 0.490 | 0.75119 |
| 4 | OtimizaÃ§Ã£o incremental | F1â‰ˆ0.766, Recall=0.781 | ajustado | 0.76315 |
| 5 | Ensemble Voting (soft, pesos ajustados) | F1=0.792, Recall=0.789 | 0.460 | 0.74880 |
| 6 | Ajuste de pesos iguais no voting | F1=0.790, Recall=0.795 | 0.450 | 0.74641 |
| 7 | Ajuste p/ maior precisÃ£o | F1=0.782, Recall=0.766 | 0.450 | 0.76315 |
| 8 | XGBoost otimizado | F1=0.795, Recall=0.804 | 0.490 | 0.75598 |
| 9 | LightGBM otimizado | F1â‰ˆ0.79 (CV) | ajustado | 0.75358 |
| 10 | Blend LogReg + GradBoost + LightGBM (1-1-2) | F1=0.798, Recallâ‰ˆ0.79 | 0.54 | **0.75837** |

---

**ðŸ“ˆ ObservaÃ§Ãµes finais:**
- A **maior pontuaÃ§Ã£o** foi **0.76315** com abordagem de ajuste incremental.
- O *blend final* superou a mÃ©dia das submissÃµes intermediÃ¡rias, mostrando potencial de ensembles mais refinados.
- O uso de **tuning de threshold** foi decisivo para equilibrar recall e precisÃ£o.
- LightGBM e XGBoost se destacaram como modelos mais estÃ¡veis.
